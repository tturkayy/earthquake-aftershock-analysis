{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10d133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style and parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9f57cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CSV files:\n",
      "1. van_earthquake.csv\n",
      "2. aegean_earthquake.csv\n",
      "3. kahramanmaras_earthquake.csv\n",
      "4. golcuk_earthquake.csv\n",
      "5. tohoku_earthquake.csv\n",
      "Select one of the source files above (1-5): 1\n",
      "Error: 'van_earthquake.csv' file not found!\n",
      "Available files: []\n",
      "Selected file: van_earthquake.csv\n"
     ]
    }
   ],
   "source": [
    "# Define available earthquake CSV files\n",
    "csv_files = {\n",
    "    '1': 'van_earthquake.csv',\n",
    "    '2': 'aegean_earthquake.csv',\n",
    "    '3': 'kahramanmaras_earthquake.csv',\n",
    "    '4': 'golcuk_earthquake.csv',\n",
    "    '5': 'tohoku_earthquake.csv'\n",
    "}\n",
    "\n",
    "# Display available files to user\n",
    "print(\"Available CSV files:\")\n",
    "for key, value in csv_files.items():\n",
    "    print(f\"{key}. {value}\")\n",
    "\n",
    "# Get user selection\n",
    "selected = input(\"Select one of the source files above (1-5): \").strip()\n",
    "\n",
    "# Validate selection\n",
    "if selected not in csv_files:\n",
    "    print(\"Invalid selection! Using 'van_earthquake.csv' as default.\")\n",
    "    selected = '1'\n",
    "\n",
    "selected_file = csv_files[selected]\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(selected_file):\n",
    "    print(f\"Error: '{selected_file}' file not found!\")\n",
    "    print(\"Available files:\", [f for f in csv_files.values() if os.path.exists(f)])\n",
    "    exit()\n",
    "\n",
    "print(f\"Selected file: {selected_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2b15d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: name 'pd' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     exit()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Clean column names (lowercase and replace spaces)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Convert and clean important columns\u001b[39;00m\n\u001b[0;32m     18\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m, utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Read CSV file with error handling for different encodings\n",
    "try:\n",
    "    df = pd.read_csv(selected_file, encoding='utf-8', low_memory=False)\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df = pd.read_csv(selected_file, encoding='latin-1', low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"CSV reading error: {e}\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Clean column names (lowercase and replace spaces)\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Convert and clean important columns\n",
    "df['time_datetime'] = pd.to_datetime(df['time'], errors='coerce', utc=True)\n",
    "df['magnitude'] = pd.to_numeric(df['mag'], errors='coerce')\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "\n",
    "# Filter out rows with missing datetime or magnitude\n",
    "valid_df = df[df['time_datetime'].notnull() & df['magnitude'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the main shock (largest magnitude earthquake)\n",
    "main_shock_idx = valid_df['magnitude'].idxmax()\n",
    "main_shock = valid_df.loc[main_shock_idx]\n",
    "main_shock_time = main_shock['time_datetime']\n",
    "main_shock_mag = main_shock['magnitude']\n",
    "\n",
    "print(f\"Main shock: {main_shock_time}, M{main_shock_mag:.1f}\")\n",
    "\n",
    "# Calculate days after main shock for each earthquake\n",
    "valid_df['days_after_main_shock'] = (\n",
    "    valid_df['time_datetime'] - main_shock_time\n",
    ").dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Filter for aftershocks (after main shock)\n",
    "aftershocks_df = valid_df[valid_df['days_after_main_shock'] > 0].copy()\n",
    "\n",
    "# Prepare daily counts for Omori's law analysis\n",
    "aftershocks_df['day_int'] = np.floor(aftershocks_df['days_after_main_shock'])\n",
    "daily_counts = aftershocks_df['day_int'].value_counts().sort_index()\n",
    "\n",
    "daily_df = pd.DataFrame({\n",
    "    'day': daily_counts.index,\n",
    "    'count': daily_counts.values\n",
    "})\n",
    "\n",
    "daily_df = daily_df[daily_df['day'] >= 0]\n",
    "daily_df['day_for_fit'] = daily_df['day'] + 1  # Avoid division by zero\n",
    "\n",
    "print(f\"Total aftershocks: {len(aftershocks_df)}\")\n",
    "print(f\"Days analyzed: {len(daily_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42844daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Omori's law function: n(t) = k/(c+t)^p\n",
    "def omori_law(t, k, c, p):\n",
    "    return k / (c + t) ** p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79493885",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if we have enough data\n",
    "    if len(daily_df) < 3:\n",
    "        raise ValueError(\"Not enough data for Omori's Law (at least 3 days required)\")\n",
    "\n",
    "    # Initial parameter guesses for curve fitting\n",
    "    initial_params = [daily_df['count'].max() * 2, 0.1, 1.0]\n",
    "\n",
    "    # Fit Omori's law to the data\n",
    "    params, covariance = curve_fit(\n",
    "        omori_law,\n",
    "        daily_df['day_for_fit'],\n",
    "        daily_df['count'],\n",
    "        p0=initial_params,\n",
    "        maxfev=10000\n",
    "    )[:2]\n",
    "\n",
    "    # Extract fitted parameters\n",
    "    k_fit, c_fit, p_fit = params\n",
    "\n",
    "    # Calculate predicted values\n",
    "    daily_df['predicted_count'] = omori_law(\n",
    "        daily_df['day_for_fit'], k_fit, c_fit, p_fit\n",
    "    )\n",
    "\n",
    "    # Calculate R-squared for goodness of fit\n",
    "    ss_res = np.sum((daily_df['count'] - daily_df['predicted_count']) ** 2)\n",
    "    ss_tot = np.sum((daily_df['count'] - np.mean(daily_df['count'])) ** 2)\n",
    "    r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "\n",
    "    # Calculate correlation\n",
    "    correlation, p_value = stats.pearsonr(\n",
    "        daily_df['count'], daily_df['predicted_count']\n",
    "    )\n",
    "\n",
    "    # Create 2x2 subplot figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Plot 1: Daily aftershock count (linear scale)\n",
    "    axes[0, 0].plot(\n",
    "        daily_df['day'], daily_df['count'], 'o-', alpha=0.7,\n",
    "        label='Actual', markersize=4\n",
    "    )\n",
    "    axes[0, 0].plot(\n",
    "        daily_df['day'], daily_df['predicted_count'], 'r--',\n",
    "        label=f'Omori (p={p_fit:.2f})', linewidth=2\n",
    "    )\n",
    "    axes[0, 0].set_xlabel('Days After Main Shock')\n",
    "    axes[0, 0].set_ylabel('Daily Aftershock Count')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Log-log scale for better visualization of power law\n",
    "    axes[0, 1].loglog(\n",
    "        daily_df['day_for_fit'], daily_df['count'], 'o',\n",
    "        alpha=0.7, label='Actual'\n",
    "    )\n",
    "    axes[0, 1].loglog(\n",
    "        daily_df['day_for_fit'], daily_df['predicted_count'], 'r--',\n",
    "        label=f'Omori (p={p_fit:.2f})', linewidth=2\n",
    "    )\n",
    "    axes[0, 1].set_xlabel('Days After Main Shock (Log)')\n",
    "    axes[0, 1].set_ylabel('Aftershock Count (Log)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Residual analysis (actual - predicted)\n",
    "    daily_df['residual'] = daily_df['count'] - daily_df['predicted_count']\n",
    "    axes[1, 0].plot(daily_df['day'], daily_df['residual'], 'o', alpha=0.7)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Days After Main Shock')\n",
    "    axes[1, 0].set_ylabel('Residual (Actual - Predicted)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Magnitude-time distribution scatter plot\n",
    "    axes[1, 1].scatter(\n",
    "        aftershocks_df['days_after_main_shock'],\n",
    "        aftershocks_df['magnitude'],\n",
    "        alpha=0.6, s=15, c='green'\n",
    "    )\n",
    "    axes[1, 1].set_xlabel('Days After Main Shock')\n",
    "    axes[1, 1].set_ylabel('Magnitude (M)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Main title with results\n",
    "    title_text = (\n",
    "        f'Earthquake Aftershock Analysis - {selected_file[:-4].upper()} (M{main_shock_mag:.1f})\\n'\n",
    "        f'Omori Law: n(t) = {k_fit:.1f}/({c_fit:.3f}+t)^{p_fit:.3f}, RÂ² = {r_squared:.3f}'\n",
    "    )\n",
    "    plt.suptitle(title_text, fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f'omori_analysis_{selected_file[:-4]}_results.png',\n",
    "        dpi=300, bbox_inches='tight'\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Model fitting error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb052df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d175a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
